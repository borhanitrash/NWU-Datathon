{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd010ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-07T05:53:10.848252Z",
     "iopub.status.busy": "2025-11-07T05:53:10.847911Z",
     "iopub.status.idle": "2025-11-07T11:06:47.260085Z",
     "shell.execute_reply": "2025-11-07T11:06:47.258193Z"
    },
    "papermill": {
     "duration": 18816.422102,
     "end_time": "2025-11-07T11:06:47.265666",
     "exception": false,
     "start_time": "2025-11-07T05:53:10.843564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Attempting to load and clean: /kaggle/input/nwu-data/NWU_CSE_FEST_2025_DATATHON_COMPETITION/train_fraud_labels.json\n",
      "JSON loaded successfully after cleaning.\n",
      "Starting feature engineering...\n",
      "Feature engineering complete.\n",
      "Setting categorical features...\n",
      "Memory usage of dataframe is 2226.87 MB\n",
      "Memory usage after optimization is: 631.90 MB\n",
      "Decreased by 71.6%\n",
      "Memory usage of dataframe is 952.42 MB\n",
      "Memory usage after optimization is: 268.86 MB\n",
      "Decreased by 71.8%\n",
      "Training with 54 features on 6240474 samples.\n",
      "Scale Pos Weight: 667.72\n",
      "========== Fold 1 ==========\n",
      "========== Fold 2 ==========\n",
      "========== Fold 3 ==========\n",
      "========== Fold 4 ==========\n",
      "========== Fold 5 ==========\n",
      "Finding optimal threshold for Cohen's Kappa...\n",
      "Overall OOF Cohen's Kappa: 0.92955 at threshold 0.693\n",
      "Creating submission file...\n",
      "Submission file 'submission.csv' created successfully!\n",
      "   transaction_id fraud\n",
      "0       8677815.0    No\n",
      "1      18228653.0    No\n",
      "2      11775845.0    No\n",
      "3      11156207.0    No\n",
      "4      15615886.0    No\n",
      "Predicted fraud distribution:\n",
      "fraud\n",
      "No     0.998618\n",
      "Yes    0.001382\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration Class\n",
    "class CFG:\n",
    "    DATA_PATH = \"/kaggle/input/nwu-data/NWU_CSE_FEST_2025_DATATHON_COMPETITION/\"\n",
    "    N_SPLITS = 5\n",
    "    SEED = 42\n",
    "    TARGET_COL = 'fraud'\n",
    "\n",
    "# Provided function to load the tricky JSON file\n",
    "def robust_load_json_with_clean(file_path):\n",
    "    \"\"\" Loads a JSON file, cleaning newline characters that might break standard parsers. \"\"\"\n",
    "    print(f\"Attempting to load and clean: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            raw_content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"FATAL ERROR: File not found at {file_path}\")\n",
    "        raise\n",
    "    # This cleaning step is crucial for the provided labels file\n",
    "    cleaned_content = raw_content.replace('\\n', '')\n",
    "    try:\n",
    "        data = json.loads(cleaned_content)\n",
    "        print(\"JSON loaded successfully after cleaning.\")\n",
    "        return data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"\\nFATAL JSON DECODE ERROR after cleaning: {e}\")\n",
    "        raise\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\" Iterate through all the columns of a dataframe and modify the data type to reduce memory usage. \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object and col_type.name != 'category' and 'datetime' not in str(col_type):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\" Creates new features for the model from the transaction data. \"\"\"\n",
    "    print(\"Starting feature engineering...\")\n",
    "    \n",
    "    # 1. Date/Time Features\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['dt_year'] = df['date'].dt.year\n",
    "    df['dt_month'] = df['date'].dt.month\n",
    "    df['dt_day'] = df['date'].dt.day\n",
    "    df['dt_hour'] = df['date'].dt.hour\n",
    "    df['dt_minute'] = df['date'].dt.minute\n",
    "    df['dt_dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['dt_dayofyear'] = df['date'].dt.dayofyear\n",
    "    df['is_weekend'] = (df['dt_dayofweek'] >= 5).astype(int)\n",
    "    \n",
    "    # 2. Amount Features\n",
    "    df['amount'] = df['amount'].replace({'\\$': ''}, regex=True).astype(float)\n",
    "    df['is_refund'] = (df['amount'] < 0).astype(int)\n",
    "    df['log_amount'] = np.log1p(df['amount'].abs())\n",
    "    \n",
    "    # 3. Categorical Features Preprocessing\n",
    "    # Fill NaNs before creating aggregations or label encoding\n",
    "    for col in ['merchant_city', 'merchant_state', 'zip', 'errors']:\n",
    "        df[col] = df[col].fillna('MISSING')\n",
    "    \n",
    "    # Additional features\n",
    "    df['has_error'] = (df['errors'] != 'MISSING').astype(int)\n",
    "    df['is_online'] = (df['merchant_city'] == 'ONLINE').astype(int)\n",
    "    \n",
    "    # 4. Aggregation Features (Client-based)\n",
    "    # These features describe the typical behavior of a client\n",
    "    df['client_avg_amount'] = df.groupby('client_id')['amount'].transform('mean')\n",
    "    df['client_std_amount'] = df.groupby('client_id')['amount'].transform('std')\n",
    "    df['client_median_amount'] = df.groupby('client_id')['amount'].transform('median')\n",
    "    df['client_transaction_count'] = df.groupby('client_id')['transaction_id'].transform('count')\n",
    "    df['client_distinct_merchant_count'] = df.groupby('client_id')['merchant_id'].transform('nunique')\n",
    "    df['client_unique_cities'] = df.groupby('client_id')['merchant_city'].transform('nunique')\n",
    "    df['client_unique_states'] = df.groupby('client_id')['merchant_state'].transform('nunique')\n",
    "    \n",
    "    # 5. Interaction Features\n",
    "    # How does this transaction's amount compare to the client's average?\n",
    "    df['amount_to_avg_ratio'] = df['amount'] / (df['client_avg_amount'] + 1e-6)\n",
    "    df['amount_to_median_ratio'] = df['amount'] / (df['client_median_amount'] + 1e-6)\n",
    "    \n",
    "    # 6. Time Delta Features\n",
    "    # Time since the client's first transaction\n",
    "    df['client_first_transaction'] = df.groupby('client_id')['date'].transform('min')\n",
    "    df['time_since_first_transaction'] = (df['date'] - df['client_first_transaction']).dt.total_seconds()\n",
    "    \n",
    "    # Time since the client's last transaction and other sorted features\n",
    "    df_sorted = df.sort_values(['client_id', 'date'])\n",
    "    df_sorted['time_since_last_transaction'] = df_sorted.groupby('client_id')['date'].diff().dt.total_seconds()\n",
    "    df_sorted['transaction_order'] = df_sorted.groupby('client_id').cumcount() + 1\n",
    "    df = df.merge(df_sorted[['transaction_id', 'time_since_last_transaction', 'transaction_order']], on='transaction_id', how='left')\n",
    "    df['time_since_last_transaction'] = df['time_since_last_transaction'].fillna(0)  # First transaction has no prior one\n",
    "    \n",
    "    # Client total span and avg interval\n",
    "    df['client_total_span'] = df.groupby('client_id')['time_since_first_transaction'].transform('max')\n",
    "    df['client_avg_interval'] = df['client_total_span'] / (df['client_transaction_count'] - 1 + 1e-6)\n",
    "    df['interval_deviation'] = df['time_since_last_transaction'] / (df['client_avg_interval'] + 1e-6)\n",
    "    \n",
    "    # Transaction fraction\n",
    "    df['transaction_fraction'] = df['transaction_order'] / df['client_transaction_count']\n",
    "    \n",
    "    # 7. Merchant-based features\n",
    "    df['merchant_popularity'] = df.groupby('merchant_id')['client_id'].transform('nunique')\n",
    "    df['merchant_avg_amount'] = df.groupby('merchant_id')['amount'].transform('mean')\n",
    "    df['amount_to_merchant_avg'] = df['amount'] / (df['merchant_avg_amount'] + 1e-6)\n",
    "    \n",
    "    # 8. Additional Shift and Novelty Features on sorted df\n",
    "    # Novelty features (first time with merchant/city/state/mcc)\n",
    "    df_sorted['is_new_merchant'] = (df_sorted.groupby(['client_id', 'merchant_id'])['date'].transform('min') == df_sorted['date']).astype(int)\n",
    "    df_sorted['is_new_city'] = (df_sorted.groupby(['client_id', 'merchant_city'])['date'].transform('min') == df_sorted['date']).astype(int)\n",
    "    df_sorted['is_new_state'] = (df_sorted.groupby(['client_id', 'merchant_state'])['date'].transform('min') == df_sorted['date']).astype(int)\n",
    "    df_sorted['is_new_mcc'] = (df_sorted.groupby(['client_id', 'mcc'])['date'].transform('min') == df_sorted['date']).astype(int)\n",
    "    \n",
    "    # Shift features (comparison to previous transaction)\n",
    "    df_sorted['prev_merchant'] = df_sorted.groupby('client_id')['merchant_id'].shift(1)\n",
    "    df_sorted['is_same_merchant'] = (df_sorted['merchant_id'] == df_sorted['prev_merchant']).astype(int)\n",
    "    \n",
    "    df_sorted['prev_city'] = df_sorted.groupby('client_id')['merchant_city'].shift(1)\n",
    "    df_sorted['is_same_city'] = (df_sorted['merchant_city'] == df_sorted['prev_city']).astype(int)\n",
    "    \n",
    "    df_sorted['prev_use_chip'] = df_sorted.groupby('client_id')['use_chip'].shift(1)\n",
    "    df_sorted['is_same_use_chip'] = (df_sorted['use_chip'] == df_sorted['prev_use_chip']).astype(int)\n",
    "    \n",
    "    df_sorted['prev_amount'] = df_sorted.groupby('client_id')['amount'].shift(1).fillna(0)\n",
    "    df_sorted['amount_ratio_prev'] = df_sorted['amount'] / (df_sorted['prev_amount'] + 1e-6)\n",
    "    \n",
    "    # Running average excluding current\n",
    "    df_sorted['cum_amount'] = df_sorted.groupby('client_id')['amount'].cumsum()\n",
    "    df_sorted['cum_count'] = df_sorted.groupby('client_id')['transaction_id'].cumcount() + 1\n",
    "    df_sorted['running_sum_prev'] = df_sorted['cum_amount'] - df_sorted['amount']\n",
    "    df_sorted['running_count_prev'] = df_sorted['cum_count'] - 1\n",
    "    df_sorted['running_avg_prev'] = np.where(df_sorted['running_count_prev'] > 0, df_sorted['running_sum_prev'] / df_sorted['running_count_prev'], 0)\n",
    "    df_sorted['amount_to_running_avg'] = df_sorted['amount'] / (df_sorted['running_avg_prev'] + 1e-6)\n",
    "    \n",
    "    # Merge new features back to original df\n",
    "    new_cols = [\n",
    "        'is_new_merchant', 'is_new_city', 'is_new_state', 'is_new_mcc',\n",
    "        'is_same_merchant', 'is_same_city', 'is_same_use_chip',\n",
    "        'amount_ratio_prev', 'running_avg_prev', 'amount_to_running_avg'\n",
    "    ]\n",
    "    df = df.merge(df_sorted[['transaction_id'] + new_cols], on='transaction_id', how='left')\n",
    "    \n",
    "    # 9. Mode-based features (usual city, state, use_chip)\n",
    "    client_mode_city = df_sorted.groupby('client_id')['merchant_city'].agg(lambda x: x.mode()[0] if not x.mode().empty else 'MISSING')\n",
    "    df['client_mode_city'] = df['client_id'].map(client_mode_city)\n",
    "    df['is_mode_city'] = (df['merchant_city'] == df['client_mode_city']).astype(int)\n",
    "    \n",
    "    client_mode_state = df_sorted.groupby('client_id')['merchant_state'].agg(lambda x: x.mode()[0] if not x.mode().empty else 'MISSING')\n",
    "    df['client_mode_state'] = df['client_id'].map(client_mode_state)\n",
    "    df['is_mode_state'] = (df['merchant_state'] == df['client_mode_state']).astype(int)\n",
    "    \n",
    "    client_mode_use_chip = df_sorted.groupby('client_id')['use_chip'].agg(lambda x: x.mode()[0] if not x.mode().empty else 'MISSING')\n",
    "    df['client_mode_use_chip'] = df['client_id'].map(client_mode_use_chip)\n",
    "    df['is_mode_use_chip'] = (df['use_chip'] == df['client_mode_use_chip']).astype(int)\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    df = df.drop(['date', 'client_first_transaction', 'client_mode_city', 'client_mode_state', 'client_mode_use_chip'], axis=1, errors='ignore')\n",
    "    \n",
    "    print(\"Feature engineering complete.\")\n",
    "    return df\n",
    "\n",
    "def run_training():\n",
    "    \"\"\" Main function to orchestrate the loading, feature engineering, training, and prediction pipeline. \"\"\"\n",
    "    \n",
    "    # --- 1. Load Data ---\n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_csv(CFG.DATA_PATH + 'train_transactions_data.csv')\n",
    "    test_df = pd.read_csv(CFG.DATA_PATH + 'test_transactions_data.csv')\n",
    "    labels_json = robust_load_json_with_clean(CFG.DATA_PATH + 'train_fraud_labels.json')\n",
    "    labels_df = pd.DataFrame(labels_json['target'].items(), columns=['transaction_id', CFG.TARGET_COL])\n",
    "    labels_df['transaction_id'] = labels_df['transaction_id'].astype(float)\n",
    "    train_df = pd.merge(train_df, labels_df, on='transaction_id', how='left')\n",
    "    \n",
    "    # Map target to 0/1\n",
    "    train_df[CFG.TARGET_COL] = train_df[CFG.TARGET_COL].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # For memory efficiency, especially since fraud is rare, we can drop rows with no label\n",
    "    train_df.dropna(subset=[CFG.TARGET_COL], inplace=True)\n",
    "    train_df[CFG.TARGET_COL] = train_df[CFG.TARGET_COL].astype(np.int8)\n",
    "    \n",
    "    # Store test transaction_ids for submission\n",
    "    test_ids = test_df['transaction_id']\n",
    "    \n",
    "    # --- 2. Feature Engineering ---\n",
    "    # Combine train and test for consistent feature creation\n",
    "    combined_df = pd.concat([train_df.drop(CFG.TARGET_COL, axis=1), test_df], ignore_index=True)\n",
    "    combined_df = feature_engineering(combined_df)\n",
    "    \n",
    "    # --- 3. Set Categorical Features ---\n",
    "    print(\"Setting categorical features...\")\n",
    "    categorical_cols = ['use_chip', 'merchant_city', 'merchant_state', 'zip', 'mcc', 'errors']\n",
    "    for col in categorical_cols:\n",
    "        combined_df[col] = combined_df[col].astype('category')\n",
    "    \n",
    "    # --- 4. Final Data Preparation ---\n",
    "    train_df_fe = combined_df.iloc[:len(train_df)]\n",
    "    test_df_fe = combined_df.iloc[len(train_df):]\n",
    "    \n",
    "    # Add target back to the training set\n",
    "    train_df_fe[CFG.TARGET_COL] = train_df[CFG.TARGET_COL].values\n",
    "    \n",
    "    # Reduce memory usage\n",
    "    train_df_fe = reduce_mem_usage(train_df_fe)\n",
    "    test_df_fe = reduce_mem_usage(test_df_fe)\n",
    "    \n",
    "    # Identify feature columns\n",
    "    features = [col for col in test_df_fe.columns if col not in ['transaction_id', CFG.TARGET_COL]]\n",
    "    X = train_df_fe[features]\n",
    "    y = train_df_fe[CFG.TARGET_COL]\n",
    "    X_test = test_df_fe[features]\n",
    "    \n",
    "    print(f\"Training with {len(features)} features on {len(X)} samples.\")\n",
    "    \n",
    "    del train_df, test_df, labels_df, combined_df, train_df_fe, test_df_fe\n",
    "    gc.collect()\n",
    "    \n",
    "    # --- 5. Model Training (LGBM with Stratified K-Fold) ---\n",
    "    skf = StratifiedKFold(n_splits=CFG.N_SPLITS, shuffle=True, random_state=CFG.SEED)\n",
    "    oof_preds = np.zeros(len(X))\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "    \n",
    "    # Calculate scale_pos_weight for imbalanced classes\n",
    "    scale_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "    print(f\"Scale Pos Weight: {scale_pos_weight:.2f}\")\n",
    "    \n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 10000,\n",
    "        'learning_rate': 0.02,\n",
    "        'num_leaves': 128,\n",
    "        'max_depth': -1,\n",
    "        'seed': CFG.SEED,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'subsample': 0.8,\n",
    "        'reg_alpha': 0.5,\n",
    "        'reg_lambda': 1.0,\n",
    "        'min_child_samples': 50,\n",
    "        'min_child_weight': 0.01,\n",
    "        'scale_pos_weight': scale_pos_weight,  # Crucial for imbalanced data\n",
    "    }\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"========== Fold {fold+1} ==========\")\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**lgb_params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric='auc',\n",
    "            callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "            categorical_feature=categorical_cols\n",
    "        )\n",
    "        \n",
    "        val_preds = model.predict_proba(X_val)[:, 1]\n",
    "        oof_preds[val_idx] = val_preds\n",
    "        \n",
    "        # Ensemble predictions for the test set\n",
    "        test_preds += model.predict_proba(X_test)[:, 1] / CFG.N_SPLITS\n",
    "        \n",
    "        del X_train, y_train, X_val, y_val, model\n",
    "        gc.collect()\n",
    "    \n",
    "    # --- 6. Find Optimal Threshold for Cohen's Kappa ---\n",
    "    print(\"Finding optimal threshold for Cohen's Kappa...\")\n",
    "    thresholds = np.linspace(0.01, 0.99, 100)\n",
    "    kappa_scores = [cohen_kappa_score(y, (oof_preds > t).astype(int)) for t in thresholds]\n",
    "    best_threshold = thresholds[np.argmax(kappa_scores)]\n",
    "    best_kappa = max(kappa_scores)\n",
    "    print(f\"Overall OOF Cohen's Kappa: {best_kappa:.5f} at threshold {best_threshold:.3f}\")\n",
    "    \n",
    "    # --- 7. Create Submission File ---\n",
    "    print(\"Creating submission file...\")\n",
    "    final_preds = (test_preds > best_threshold).astype(int)\n",
    "    submission = pd.DataFrame({\n",
    "        'transaction_id': test_ids,\n",
    "        CFG.TARGET_COL: final_preds\n",
    "    })\n",
    "    submission[CFG.TARGET_COL] = submission[CFG.TARGET_COL].map({1: 'Yes', 0: 'No'})\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission file 'submission.csv' created successfully!\")\n",
    "    print(submission.head())\n",
    "    print(f\"Predicted fraud distribution:\\n{submission[CFG.TARGET_COL].value_counts(normalize=True)}\")\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14353069,
     "sourceId": 119941,
     "sourceType": "competition"
    },
    {
     "datasetId": 8655376,
     "sourceId": 13619404,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18823.342548,
   "end_time": "2025-11-07T11:06:48.914181",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-07T05:53:05.571633",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
